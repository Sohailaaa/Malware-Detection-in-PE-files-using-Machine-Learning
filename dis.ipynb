{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Jaccard index: 0.977251\n",
      "Random Forest F1-score: 0.988811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RF_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal rf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Load the extracted PE Headers csv file from the PE files dataset.\n",
    "pe_df = pd.read_csv(r\".\\PE_Header(exe, dll files)\\final_pe_data.csv\")\n",
    "\n",
    "# From the csv file, load the part of the dataframe so that it is class balanced.\n",
    "final_pe_df = pd.concat([pe_df[:20003], pe_df[116047:]], ignore_index=True)\n",
    "final_pe_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "X = final_pe_df.drop(['legitimate'], axis=1).values\n",
    "y = final_pe_df['legitimate'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "RF_model = RandomForestClassifier(n_estimators=50)\n",
    "RF_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "RF_yhat = RF_model.predict(X_test)\n",
    "print(\"Random Forest Jaccard index: %.6f\" % jaccard_score(y_test, RF_yhat))\n",
    "print(\"Random Forest F1-score: %.6f\" % f1_score(y_test, RF_yhat, average='weighted'))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(RF_model, 'RF_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Jaccard index: 0.976586\n",
      "XGBoost F1-score: 0.988493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['XGB_model.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal XGBOOST\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Load the extracted PE Headers csv file from the PE files dataset.\n",
    "pe_df = pd.read_csv(r\".\\PE_Header(exe, dll files)\\final_pe_data.csv\")\n",
    "\n",
    "# From the csv file, load the part of the dataframe so that it is class balanced.\n",
    "final_pe_df = pd.concat([pe_df[:20003], pe_df[116047:]], ignore_index=True)\n",
    "final_pe_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "X = final_pe_df.drop(['legitimate'], axis=1).values\n",
    "y = final_pe_df['legitimate'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "XGB_model = XGBClassifier(max_depth=20, learning_rate=0.3, n_estimators=150)\n",
    "XGB_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "XGB_yhat = XGB_model.predict(X_test)\n",
    "print(\"XGBoost Jaccard index: %.6f\" % jaccard_score(y_test, XGB_yhat))\n",
    "print(\"XGBoost F1-score: %.6f\" % f1_score(y_test, XGB_yhat, average='weighted'))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(XGB_model, 'XGB_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0170667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ngrams(byte, asm files)/asmfiles.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rf with spark pe headers\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomForestClassifierExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data into a Spark DataFrame directly\n",
    "pe_df_spark = spark.read.csv(\"PE_Header(exe, dll files)/final_pe_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Normalize the data and create features column\n",
    "assembler = VectorAssembler(inputCols=pe_df_spark.columns[:-1], outputCol=\"features\")\n",
    "pe_df_spark = assembler.transform(pe_df_spark)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_data, test_data) = pe_df_spark.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"legitimate\", featuresCol=\"features\", numTrees=50)\n",
    "RF_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = RF_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"legitimate\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\"Ngrams(byte, asm files)/asmfiles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20109\\Pictures\\ml\\myenv\\Lib\\site-packages\\dask_ml\\model_selection\\_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dask with rf bytes &Asm files\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.metrics import accuracy_score\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Step 1: Read the dataset as a Dask DataFrame\n",
    "ddf = dd.read_csv(\"Ngrams(byte, asm files)/merged.csv\")\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "X = ddf.drop(columns=['Malware'])\n",
    "y = ddf['Malware']\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Preprocessing\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays\n",
    "X_train, y_train = X_train.compute(), y_train.compute()\n",
    "X_test, y_test = X_test.compute(), y_test.compute()\n",
    "\n",
    "# Check if arrays are empty\n",
    "if len(X_train) == 0 or len(X_test) == 0:\n",
    "    raise ValueError(\"Empty arrays detected.\")\n",
    "\n",
    "# Step 5: Train the Random Forest model\n",
    "rf_model = ParallelPostFit(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = rf_model._postfit_estimator.predict(X_test.values)\n",
    "accuracy = accuracy_score(y_test.values, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection as dms\n",
    "import dask_ml.metrics as dmm\n",
    "import dask_ml.impute as dmi\n",
    "import dask_ml.preprocessing as dmp\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from dask.delayed import delayed\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "\n",
    "# Step 1: Read the dataset as a Dask DataFrame\n",
    "ddf = dd.read_csv(\"Ngrams(byte, asm files)/merged.csv\")\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "X = ddf.drop(columns=['Malware'])\n",
    "y = ddf['Malware']\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = dms.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Preprocessing\n",
    "X_train_arr = X_train.compute().to_numpy()\n",
    "X_test_arr = X_test.compute().to_numpy()\n",
    "\n",
    "imputer = dmi.SimpleImputer(strategy='mean')\n",
    "X_train_arr = imputer.fit_transform(X_train_arr)\n",
    "X_test_arr = imputer.transform(X_test_arr)\n",
    "\n",
    "scaler = dmp.StandardScaler()\n",
    "X_train_arr = scaler.fit_transform(X_train_arr)\n",
    "X_test_arr = scaler.transform(X_test_arr)\n",
    "\n",
    "# Create Dask DataFrames using from_delayed\n",
    "X_train_delayed = [delayed(lambda x, i=i: x, X_train_arr[i]) for i in range(X_train_arr.shape[0])]\n",
    "X_train = dd.from_delayed(X_train_delayed, dtype=X.dtype, npartitions=X_train.npartitions)\n",
    "\n",
    "X_test_delayed = [delayed(lambda x, i=i: x, X_test_arr[i]) for i in range(X_test_arr.shape[0])]\n",
    "X_test = dd.from_delayed(X_test_delayed, dtype=X.dtype, npartitions=X_test.npartitions)\n",
    "\n",
    "# Step 5: Train the XGBoost model\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "bst = xgb.dask.train(client, {'objective': 'binary:logistic'}, dtrain, num_boost_round=10)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = xgb.dask.predict(client, bst, X_test)\n",
    "accuracy = dmm.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
