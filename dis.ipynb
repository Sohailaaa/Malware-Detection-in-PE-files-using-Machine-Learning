{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Jaccard index: 0.977251\n",
      "Random Forest F1-score: 0.988811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RF_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal rf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Load the extracted PE Headers csv file from the PE files dataset.\n",
    "pe_df = pd.read_csv(r\".\\PE_Header(exe, dll files)\\final_pe_data.csv\")\n",
    "\n",
    "# From the csv file, load the part of the dataframe so that it is class balanced.\n",
    "final_pe_df = pd.concat([pe_df[:20003], pe_df[116047:]], ignore_index=True)\n",
    "final_pe_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "X = final_pe_df.drop(['legitimate'], axis=1).values\n",
    "y = final_pe_df['legitimate'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "RF_model = RandomForestClassifier(n_estimators=50)\n",
    "RF_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "RF_yhat = RF_model.predict(X_test)\n",
    "print(\"Random Forest Jaccard index: %.6f\" % jaccard_score(y_test, RF_yhat))\n",
    "print(\"Random Forest F1-score: %.6f\" % f1_score(y_test, RF_yhat, average='weighted'))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(RF_model, 'RF_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Jaccard index: 0.979493\n",
      "Random Forest F1-score: 0.989132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RF_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal rf with ds2\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Load the extracted PE Headers csv file from the PE files dataset.\n",
    "pe_df = pd.read_csv(r\".\\ds2.csv\")\n",
    "\n",
    "# From the csv file, load the part of the dataframe so that it is class balanced.\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "X = pe_df.drop(['class'], axis=1).values\n",
    "y = pe_df['class'].values\n",
    "# Perform one-hot encoding on the 'packer' column\n",
    "# Drop the 'packer_type' column from the dataframe\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "import numpy as np\n",
    "\n",
    "# Replace NaN values in y_train with 0\n",
    "y_train = np.nan_to_num(y_train, nan=0)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "RF_model = RandomForestClassifier(n_estimators=50)\n",
    "RF_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Replace NaN values in y_test with 0\n",
    "y_test = np.nan_to_num(y_test, nan=0)\n",
    "\n",
    "# Evaluate the model\n",
    "RF_yhat = RF_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Jaccard index: %.6f\" % jaccard_score(y_test, RF_yhat))\n",
    "print(\"Random Forest F1-score: %.6f\" % f1_score(y_test, RF_yhat, average='weighted'))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(RF_model, 'RF_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25652\\344636763.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Split the data into training and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Convert Pandas DataFrames back to Spark DataFrames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mX_train_spark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mX_test_spark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Convert labels to Spark DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20109\\Pictures\\ml\\myenv\\Lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[0mhas_pandas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_pandas\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[1;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m             return super(SparkSession, self).createDataFrame(\n\u001b[0m\u001b[0;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20109\\Pictures\\ml\\myenv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    295\u001b[0m                         \u001b[1;34m\"fallback with 'spark.sql.execution.arrow.pyspark.fallback.enabled' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m                         \"has been set to false.\\n  %s\" % str(e))\n\u001b[0;32m    297\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                     \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20109\\Pictures\\ml\\myenv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[0;32m    327\u001b[0m                                 \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                                 \u001b[0mcopied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                             \u001b[0mpdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_series_convert_timestamps_tz_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopied\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20109\\Pictures\\ml\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6293\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6294\u001b[0m         ):\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6296\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "import joblib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier as SparkRandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Random Forest with Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the extracted PE Headers csv file from the PE files dataset into a Spark DataFrame\n",
    "pe_df = spark.read.csv(\"ds2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# From the csv file, load the part of the dataframe so that it is class balanced.\n",
    "# You can perform any necessary data balancing here if needed.\n",
    "\n",
    "# Convert DataFrame to Pandas for preprocessing (if necessary)\n",
    "pe_df_pandas = pe_df.toPandas()\n",
    "\n",
    "# Normalize the data (if necessary)\n",
    "X = pe_df_pandas.drop(['class'], axis=1)\n",
    "y = pe_df_pandas['class']\n",
    "\n",
    "# Perform one-hot encoding on the 'packer' column and drop the 'packer_type' column from the dataframe\n",
    "# Perform any necessary preprocessing here\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Convert Pandas DataFrames back to Spark DataFrames\n",
    "X_train_spark = spark.createDataFrame(X_train)\n",
    "X_test_spark = spark.createDataFrame(X_test)\n",
    "\n",
    "# Convert labels to Spark DataFrame\n",
    "y_train_spark = spark.createDataFrame(pd.DataFrame({'class': y_train}))\n",
    "y_test_spark = spark.createDataFrame(pd.DataFrame({'class': y_test}))\n",
    "\n",
    "# Assemble features into a single column\n",
    "assembler = VectorAssembler(inputCols=X_train.columns, outputCol=\"features\")\n",
    "X_train_spark = assembler.transform(X_train_spark)\n",
    "X_test_spark = assembler.transform(X_test_spark)\n",
    "\n",
    "# Create and train the Random Forest model using Spark\n",
    "rf = SparkRandomForestClassifier(featuresCol=\"features\", labelCol=\"class\", numTrees=50)\n",
    "rf_model = rf.fit(X_train_spark)\n",
    "\n",
    "# Make predictions on the test set\n",
    "RF_yhat = rf_model.transform(X_test_spark)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(RF_yhat)\n",
    "print(\"Random Forest Accuracy: %.6f\" % accuracy)\n",
    "\n",
    "# Save the model\n",
    "rf_model.save(\"RF_model_spark\")\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.958025\n"
     ]
    }
   ],
   "source": [
    "#rf with spark pe headers\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomForestClassifierExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data into a Spark DataFrame directly\n",
    "pe_df_spark = spark.read.csv(\"ds2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Normalize the data and create features column\n",
    "assembler = VectorAssembler(inputCols=pe_df_spark.columns[:-1], outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "pe_df_spark = assembler.transform(pe_df_spark)\n",
    "# Check the schema of the DataFrame after VectorAssembler transformation\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_data, test_data) = pe_df_spark.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\", numTrees=50)\n",
    "try:\n",
    "    RF_model = rf.fit(train_data)\n",
    "except Exception as e:\n",
    "    print(\"Error during model fitting:\", e)\n",
    "    raise e  # Re-raise the exception to halt the execution\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = RF_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % accuracy)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Jaccard index: 0.976586\n",
      "XGBoost F1-score: 0.988493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['XGB_model.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal XGBOOST\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Load the extracted PE Headers csv file from the PE files dataset.\n",
    "pe_df = pd.read_csv(r\".\\PE_Header(exe, dll files)\\final_pe_data.csv\")\n",
    "\n",
    "# From the csv file, load the part of the dataframe so that it is class balanced.\n",
    "final_pe_df = pd.concat([pe_df[:20003], pe_df[116047:]], ignore_index=True)\n",
    "final_pe_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "X = final_pe_df.drop(['legitimate'], axis=1).values\n",
    "y = final_pe_df['legitimate'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "XGB_model = XGBClassifier(max_depth=20, learning_rate=0.3, n_estimators=150)\n",
    "XGB_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "XGB_yhat = XGB_model.predict(X_test)\n",
    "print(\"XGBoost Jaccard index: %.6f\" % jaccard_score(y_test, XGB_yhat))\n",
    "print(\"XGBoost F1-score: %.6f\" % f1_score(y_test, XGB_yhat, average='weighted'))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(XGB_model, 'XGB_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0170667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ngrams(byte, asm files)/asmfiles.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rf with spark pe headers\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomForestClassifierExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data into a Spark DataFrame directly\n",
    "pe_df_spark = spark.read.csv(\"PE_Header(exe, dll files)/final_pe_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Normalize the data and create features column\n",
    "assembler = VectorAssembler(inputCols=pe_df_spark.columns[:-1], outputCol=\"features\")\n",
    "pe_df_spark = assembler.transform(pe_df_spark)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_data, test_data) = pe_df_spark.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"legitimate\", featuresCol=\"features\", numTrees=50)\n",
    "RF_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = RF_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"legitimate\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\"Ngrams(byte, asm files)/asmfiles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20109\\Pictures\\ml\\myenv\\Lib\\site-packages\\dask_ml\\model_selection\\_split.py:462: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dask with rf bytes &Asm files\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.metrics import accuracy_score\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Step 1: Read the dataset as a Dask DataFrame\n",
    "ddf = dd.read_csv(\"Ngrams(byte, asm files)/merged.csv\")\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "X = ddf.drop(columns=['Malware'])\n",
    "y = ddf['Malware']\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Preprocessing\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays\n",
    "X_train, y_train = X_train.compute(), y_train.compute()\n",
    "X_test, y_test = X_test.compute(), y_test.compute()\n",
    "\n",
    "# Check if arrays are empty\n",
    "if len(X_train) == 0 or len(X_test) == 0:\n",
    "    raise ValueError(\"Empty arrays detected.\")\n",
    "\n",
    "# Step 5: Train the Random Forest model\n",
    "rf_model = ParallelPostFit(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = rf_model._postfit_estimator.predict(X_test.values)\n",
    "accuracy = accuracy_score(y_test.values, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection as dms\n",
    "import dask_ml.metrics as dmm\n",
    "import dask_ml.impute as dmi\n",
    "import dask_ml.preprocessing as dmp\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from dask.delayed import delayed\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "\n",
    "# Step 1: Read the dataset as a Dask DataFrame\n",
    "ddf = dd.read_csv(\"Ngrams(byte, asm files)/merged.csv\")\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "X = ddf.drop(columns=['Malware'])\n",
    "y = ddf['Malware']\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = dms.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Preprocessing\n",
    "X_train_arr = X_train.compute().to_numpy()\n",
    "X_test_arr = X_test.compute().to_numpy()\n",
    "\n",
    "imputer = dmi.SimpleImputer(strategy='mean')\n",
    "X_train_arr = imputer.fit_transform(X_train_arr)\n",
    "X_test_arr = imputer.transform(X_test_arr)\n",
    "\n",
    "scaler = dmp.StandardScaler()\n",
    "X_train_arr = scaler.fit_transform(X_train_arr)\n",
    "X_test_arr = scaler.transform(X_test_arr)\n",
    "\n",
    "# Create Dask DataFrames using from_delayed\n",
    "X_train_delayed = [delayed(lambda x, i=i: x, X_train_arr[i]) for i in range(X_train_arr.shape[0])]\n",
    "X_train = dd.from_delayed(X_train_delayed, dtype=X.dtype, npartitions=X_train.npartitions)\n",
    "\n",
    "X_test_delayed = [delayed(lambda x, i=i: x, X_test_arr[i]) for i in range(X_test_arr.shape[0])]\n",
    "X_test = dd.from_delayed(X_test_delayed, dtype=X.dtype, npartitions=X_test.npartitions)\n",
    "\n",
    "# Step 5: Train the XGBoost model\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "bst = xgb.dask.train(client, {'objective': 'binary:logistic'}, dtrain, num_boost_round=10)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = xgb.dask.predict(client, bst, X_test)\n",
    "accuracy = dmm.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
